---
title: "Using Poisson Regression to Model American Football Scoring and Exploit Inaccuracies in the Online Betting Market"
author: "Mofeed Nagib"
date: "12/17/2023"
output:
  pdf_document: default
  word_document: default
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE,      ## show or suppress the code
                      include = TRUE ,      ## show or suppress the output
                      message = FALSE,      ## omit messages generated by code
                      warning = FALSE,      ## omit warnings generated by code
                      comment = NA,      ## removes the ## from in front of outputs
                      fig.align = "center",      ## centers all figures
                      fig.height = 5,      ## set the default height
                      fig.weight = 5)       ## set the default width
```

```{r}
library(ggplot2)
library(corrplot)
```

## Abstract
In the landscape of NFL score forecasting, existing models often treat the score as a continuous variable, overlooking the inherent patterns of scoring in American football — frequently manifesting as combinations of multiples of 3 and 7. Moreover, these models typically predict the total points scored by each team, rather than projecting scoring events (e.g., the number of touchdowns or the number of field goals) and combining them to calculate an overall score. Given these limitations, we present an innovative bivariate Poisson regression that jointly models touchdowns and field goals, recognizing their inherent inverse relationship. More specifically, this regression was fitted as a dependent (marginal/conditional) bivariate Poisson model using the [bpglm](https://github.com/chowdhuryri/bpglm) package and employs team, opposing team, season, and home/away status as predictors. The aggregate training dataset in this project was sourced from the [nflfastR](https://CRAN.R-project.org/package=nflfastR) package and [Kaggle](https://www.kaggle.com/datasets/tobycrabtree/nfl-scores-and-betting-data), and includes game details, scoring event breakdowns, and betting and spread information spanning 1999 to the present. Our model achieves an accuracy of nearly 53% in correctly predicting both the spread and the over-under, demonstrating some efficacy in capturing the nuanced dynamics of NFL scoring.

## Executive Summary
<!-- An executive summary is typically longer than the abstract, up to a page, could possibly contain key visualizations, tables, or other figures that help communicate either the raw data or the results of the model, and is intended for someone outside of the data science/analytics team of an organization. It is important to be as concise as possible, and describe each of those points above without using language that is overly technical and not part of commonly used English. The executive summary is a separate document.  -->

<!-- Note that in the abstract, executive summary, and throughout the report you should avoid using first-person singular pronouns like "I" and "me", even if you are the only author. Use "we" or use passive voice.  -->

## Introduction

The past decade has witnessed a substantial increase in the popularity of online gambling, particularly in the realm of sports betting. Online sportsbooks like DraftKings and FanDuel have risen to prominence in the industry, with the National Football League (NFL) standing out as the most popular league to bet on. In an attempt to gain a competitive edge, many enthusiasts have turned to statistical models that analyze various factors such as player performance, team dynamics, and historical data. However, one common oversight in the majority of these models is their treatment of the game score as a continuous variable. In reality, scoring in American football often follows a distinctive pattern, characterized by combinations of multiples of 3 and 7. Furthermore, most of the these models focus on predicting the final score as a single outcome rather than forecasting the individual scoring events that contribute to it. Hence, a more nuanced exploration of score breakdown could be essential to enhancing the predictive accuracy of these models.

In the NFL, there are five possible ways to score points: touchdowns, field goals, extra points, safeties, and two-point conversions. A touchdown is the most valuable play, worth six points, achieved when a player crosses the opponent's goal line with possession of the ball or catches a pass in the end zone. Following a touchdown, teams have the option to attempt an extra point by kicking the ball through the goalposts for one additional point or go for a two-point conversion by running or passing the ball into the end zone. Field goals, worth three points, come into play when a team successfully kicks the ball through the opposing team's uprights. Safeties are rare but impactful scoring opportunities, occurring when the defense tackles an offensive player in their own end zone, awarding the defensive team two points. Hence, any points scored can be attributed to one of these five categories. It is important to note that games with defensive two-point conversions are excluded from this analysis, due to their brief history and limited occurrence.

To compile a dataset containing these scoring category breakdowns, we utilized detailed play-by-play data obtained through the [nflfastR](https://CRAN.R-project.org/package=nflfastR) package. This dataset spans from the 1999 season to the present, encompassing almost 400 variables that cover everything from play details and game specifics to team statistics and player details, and various other advanced metrics. Subsequently, this dataset was joined with game betting data sourced from [Kaggle](https://www.kaggle.com/datasets/tobycrabtree/nfl-scores-and-betting-data), which contained simple game details (date, teams, and scores, etc.), and corresponding spread and over/under lines. In the forthcoming analysis, we leverage the scoring category breakdown data for training our bivariate Poisson model, while assessing the model's quality with betting odds data to gauge its performance against Vegas.

Section 2 of this report contains the data explorations and visualizations, which show the near Poisson di


which reveal the fairly clear Poisson relationship of each scoring type. Furthermore, we have a correlation matrix plot and a few pairwise scatterplots that show some of the positive and negative correlations between number of each scoring type. In section 3, we build our several different models for this


<!-- A few paragraphs that contain the following: -->

<!-- - background on the topic you are studying, including the motivation behind the project and the problem statement you mentioned in the abstract, but in more detail -->
<!-- - a description of the data -->
<!-- - a sentence or paragraph describing what is contained in each section of the rest of the paper. Include roughly one sentence per section, that describes what is in the section and the main takeaways from each section.  For example,  -->

<!-- > "Section 2 contains data exploration and visualization, which reveals that *****.  In Section 3, we build several different predictive models and find that *****. We discuss the results of the model, including *****, in Section 4. Finally, we discuss conclusions, recommendations and ideas for future work in Section 5." -->

<!-- There are similarities between the introduction and the abstract. The introduction is longer and more detailed, especially in terms of background, previous work, and motivation of the problem, and contains a brief outline of the contents of the rest of the paper. -->



of tallying the occurrences of individual scoring events, such as the number of touchdowns and field goals, and summing those to get a predicted score. 

Moreover, these models tend to predict the score itself rather than counting the number of individual scoring events (e.g. number of touchdowns and number of field goals)

no Poisson-based models have been considered, although these models are common in other random appearing sports like soccer and hockey,

The training dataset for this model was compiled from detailed play-by-play records obtained through the [nflfastR](https://CRAN.R-project.org/package=nflfastR) package and betting odds data sourced from [Kaggle](https://www.kaggle.com/datasets/tobycrabtree/nfl-scores-and-betting-data). 

## Data exploration and visualization 

The primary data extraction and processing operations are performed in the `05_get_data.R` script, consolidating all the information that we will need to train and evaluate the model. Within this script, we iterated through play-by-play data for each game, tallying the number of touchdowns, field goals, extra points, two-point conversion attempts, and safeties. Following this, we integrated this data with the associated spread and over/under data for the respective games. Ultimately, we filtered out games where the scoring breakdown did not sum to the original final scores of the game or if the betting data is absent for that particular game. It is also important to note that this analysis excludes games featuring defensive two-point conversions, due to their brief history and limited occurrence. This data preparation concluded with aggregate data for 6,597 of the 6,630 games played since 1999, providing a comprehensive foundation for subsequent model training and evaluation. Below is a table that lists all the variables we aggregated along with their corresponding descriptions.

$$
\begin{tabular}{|c|c|c|}
\hline
Variable Name & Variable Type & Variable Description \\
\hline
game\_id & Categorical & Unique identifier for each game\\
\hline
game\_date & Numerical & Date of the game (YYYY-MM-DD)\\
\hline
week_of_season & Categorical & Week of the season (i.e. 1 - 21)\\
\hline
season & Categorical & Season of the game (YYYY)\\
\hline
home\_team & Categorical & Home team name\\
\hline
away\_team & Categorical & Away team name\\
\hline
home\_final\_score & Numerical & Home team's final score\\
\hline
away\_final\_score & Numerical & Away team's final score\\
\hline
home\_TD\_count & Numerical & Home team's touchdown count\\
\hline
away\_TD\_count & Numerical & Away team's touchdown count\\
\hline
home\_FG\_count & Numerical & Home team's field goal count\\
\hline
home\_FG\_attempts & Numerical & Home team's field goal attempt count\\
\hline
away\_FG\_count & Numerical & Away team's field goal count\\
\hline
away\_FG\_attempts & Numerical & Away team's field goal attempt count\\
\hline
home\_XP\_count & Numerical & Home team's extra point count\\
\hline
home\_XP\_attempts & Numerical & Home team's extra point attempt count\\
\hline
away\_XP\_count & Numerical & Away team's extra point count\\
\hline
away\_XP\_attempts & Numerical & Away team's extra point attempt count\\
\hline
home\_TWO\_PT\_count & Numerical & Home team's two point conversion count\\
\hline
home\_TWO\_PT\_attempts & Numerical & Home team's two point conversion attempt count\\
\hline
away\_TWO\_PT\_count & Numerical & Away team's two point conversion count\\
\hline
away\_TWO\_PT\_attempts & Numerical & Away team's two point conversion attempt count\\
\hline
home\_SAFETY\_count & Numerical & Home team's safety count\\
\hline
away\_SAFETY\_count & Numerical & Away team's safety count\\
\hline
team\_favorite\_id & Categorical & Team favorited to win the game\\
\hline
spread\_favorite & Numerical & Expected winning margin for the favored team\\
\hline
over\_under\_line & Numerical & Expected total combined score of both teams\\
\hline
\end{tabular}
$$

Although we had 




```{r}
game_betting <- readRDS("data/game_betting_data.RDS")

score_events <- data.frame(cbind(TD = c(game_betting$home_TD_count, game_betting$away_TD_count),
                                 FG = c(game_betting$home_FG_count, game_betting$away_FG_count),
                                 XP = c(game_betting$home_XP_count, game_betting$away_XP_count),
                                 TWO_PT = c(game_betting$home_TWO_PT_count, game_betting$away_TWO_PT_count),
                                 SAFETY = c(game_betting$home_SAFETY_count, game_betting$away_SAFETY_count),
                                 row.names = NULL))

corrplot.mixed(cor(score_events), upper = "ellipse")


# visualize touchdown density distribution with poisson overlay
ggplot(data = data.frame(TD = score_events$TD),
       aes(x = TD, y = after_stat(density))) +
  geom_histogram(binwidth = 1, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Distribution of Touchdowns in NFL Games (w/ Poisson Overlay)",
       x = "Number of Touchdowns",
       y = "Density") +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_x_continuous(breaks = seq(0, 10, by = 2)) +
  geom_point(data = data.frame(x = 0:10, y = dpois(0:10, lambda = mean(score_events$TD))),
             aes(x = x, y = y), color = 'red', size = 3)

# visualize field goal distribution with poisson overlay
ggplot(data = data.frame(field_goals = score_events$FG),
       aes(x = field_goals, y = after_stat(density))) +
  geom_histogram(binwidth = 1, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Distribution of Field Goals in NFL Games (w/ Poisson Overlay)",
       x = "Number of Field Goals",
       y = "Density") +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_x_continuous(breaks = seq(0, 8, by = 2)) +
  geom_point(data = data.frame(x = 0:8, y = dpois(0:8, lambda = mean(score_events$FG))),
             aes(x = x, y = y), color = 'red', size = 3)

# visualize the number of touchdowns vs. field goals (appears to be negatively correlated)
ggplot(game_betting, aes(x = home_TD_count, y = home_FG_count)) +
  geom_point(position = position_jitter(width = 0.2, height = 0.2), 
             color = "black", size = 3, alpha = 0.7, shape = 16) +
  geom_point(aes(x = away_TD_count, y = away_FG_count), 
             position = position_jitter(width = 0.2, height = 0.2), 
             color = "black", size = 3, alpha = 0.7, shape = 16) +
  labs(title = "Touchdowns vs Field Goals in NFL Games",
       x = "Number of Touchdowns",
       y = "Number of Field Goals") +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_x_continuous(breaks = seq(0, 10, by = 2))
```


This section will have descriptive statistics and visualizations of the raw data.  Use this section to reveal to the reader any interesting relationships in the data, and convince the reader that the predictors are related to the outcome. Visualizations are one of the most powerful ways to communicate information to the reader, so it is important to spend time producing clear, descriptive, eye-catching visualizations.

The package `pubtheme` has a `ggplot` theme called `theme_pub` that helps with making publication-quality visualizations with `ggplot`. See https://github.com/bmacGTPM/pubtheme. There are also several templates there that you can copy, paste, and modify. 

If you display a data visualization or some other summary of data, discuss the significance of what you see. What does this tell you about the data? What does it tell you that will help you with modeling? Do not simply show a visualization for the sake of showing a visualization.

Since `echo=F` is the option chosen at the top, the default will be to show the output but not the code: 
```{r}
1+1
```

If you don't want to show the output either, you can use `include=F`:
```{r, include = F}
1+1
```

Nothing was shown above. If you want to force it to show the code for some reason, you can override the default by putting the options `echo=T` for this chunk.
```{r, echo = T}
1+1
```

However, since this is a formal report, you will likely not want to show code. 

## Modeling/Analysis

When building our model for this project, we had hoped to use all five scoring categories to 

<!-- $$Y_{TD} = e^{\beta_{0} \:+\: \beta_{ATL}\,X_{ATL} \:+\: \beta_{BAL}\,X_{BAL} + \cdots + }$$ -->

More specifically, the joint bivariate Poisson distribution was fit as the product of a marginal and conditional Poisson distribution

Describe regression or classification model(s) used, or the analysis that was performed. For each regression or classification model, discuss 

- any assumptions that are made
- the observation, the predictors, and the outcome (aka the rows of $X$, the columns of $X$, and $y$)
- what model you are using, and write out the model
- what the coefficients mean (when applicable) and how this is related to your problem 
- appropriate measures of the performance of the model, such as measures of fit and predictive ability
- whether or not you think the model is appropriate for this kind of data, and why, and
- how easy/hard it is to interpret the results and explain them to either a technical or non-technical audience.

For other kinds of analysis, what you give is highly dependent on the type of analysis. But in general, talk about assumptions, if they are appropriate, how they might not be appropriate, and why you chose this type of analysis. 

## Visualization and interpretation of the results
Create visualizations of the results when appropriate, focusing on visualizations that 

- help describe aspects of the results that have real-world interpretation 
- help the reader understand how the model addresses the problem you are studying. 

**Visualizations are one of the most powerful ways to communicate information to the reader, so it is important to spend time producing clear, descriptive, eye-catching visualizations.**

Discuss the results of the model or models you chose, and describe how they are related to the problem statement or question that you were trying to answer in the project.  

If you have built multiple models or types of analysis, compare the measures of performance and the ease of interpretability across models or types of analysis, stating which model or models performed best, and which model or models were most interpretable.  Finally, decide which model or type of analysis is best for your particular problem based on some combination of performance and interpretability.

## Conclusions and recommendations
One or two paragraphs stating conclusions, recommendations, and ideas for future work and improvements.

## Appendix (optional)
Any supporting information or additional information that isn't necessary to have in the main body of the paper. For example, huge tables can go here, especially if they are more than one page. Tables that are, for example, 100 pages most likely should not be included at all.

## Work Cited
1. Gifford, Matt, and Tuncay Bayrak. “A predictive analytics model for forecasting outcomes in the National Football League games using decision tree and logistic regression.” *Decision Analytics Journal*, vol. 8, Sept. 2023, https://doi.org/10.1016/j.dajour.2023.100296.
2. The Factory of Sadness. “Machine Learning Model for NFL Betting (Model 5.0).” *Medium*, Medium, 15 May 2023, medium.com/@bravenewworld21/machine-learning-model-for-nfl-betting-model-5-0-8e916428c330. 
3. AlMuhayfith, Fatimah E., et al. “On bivariate Poisson Regression Models.” *Journal of King Saud University - Science*, vol. 28, no. 2, Apr. 2016, pp. 178–189, https://doi.org/10.1016/j.jksus.2015.09.003.
4. Ford, Clay. “Getting Started with Multivariate Multiple Regression.” *UVA Library StatLab*, 27 Oct. 2017, library.virginia.edu/data/articles/getting-started-with-multivariate-multiple-regression.
